{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy numerical library for numerical operations in python \n",
    "import numpy as np\n",
    "\n",
    "# import pyplot for creatic visualisations \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# try using funcanimation to cretae animations by repeatedly calling a function \n",
    "from matplotlib.animation import FuncAnimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- In this Jupyter notebook, I want to implement a naive gradient descent algorithm. \n",
    "- Note that we will begin with a simple differentiable function f(x), find its slope, then iterate to find the optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 \n",
    "\n",
    "# Define the function we want to minimize, its a simple quadratic function, where we know the minimum of the function\n",
    "\n",
    "def function(x):\n",
    "    return (x - 3) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "\n",
    "# Define the gradient of f(x), f'(x), which describes the slope of the function at any point, x\n",
    "\n",
    "def f_prime(x): \n",
    "    return 2 * (x-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 \n",
    "\"\"\"\n",
    "Design the gradient descent algorithm\n",
    "Initialize with some guess for the minimum, a learning rate a.k.a. eta (size of step to take in order to make next guess), \n",
    "number of iterations a.k.a epochs (max amount of total steps to take to arrive at correct answer)\n",
    "\"\"\"\n",
    "\n",
    "def gradient_descent(starting_x, learning_rate, num_iterations):\n",
    "    # initialise an list to store all of our guesses \n",
    "    x_history = [starting_x]\n",
    "\n",
    "    # define a loop that performs the gradient descent algorithm\n",
    "    # at each iteration, it computes the gradient of the the function at the current point, `x`\n",
    "    # then it updates `x` by taking a step in the direction opposite to  the gradient (since we are minimizing)\n",
    "    # the sice of the steis determined by `learning rate`\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        grad = f_prime(starting_x)\n",
    "        x = starting_x - learning_rate * grad\n",
    "        x_history.append(x)\n",
    "    \n",
    "    return x, x_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 \n",
    "# Call the gradient descent function  with the specified parameters\n",
    "\n",
    "starting_x = 0 \n",
    "learning_rate = 0.1 \n",
    "num_iterations = 50 \n",
    "\n",
    "minimum_x, x_history = gradient_descent(starting_x, learning_rate, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
